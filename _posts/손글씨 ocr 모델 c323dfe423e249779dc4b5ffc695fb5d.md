# 손글씨 ocr 모델

---

참조 : [https://cvml.tistory.com/16](https://cvml.tistory.com/16)

### OCR - Optical Character Recognition

- 사람이 직접 입력하지 않고 스캐너를 통해 이미지 형태로 읽어들여 데이터의 내용을 분석하고 그림 영역과 글자 영역으로 구분한 후 글자 영역의 문자들을 일반 문서 편집기에 수정, 편집이 가능한 텍스트의 형태로 변환하여 주는 자동 입력 시스템
- 과정
    - 전치리(pre-processing)
        - 글자들이 잘 보여질 수 있게 밝기나 색과 같은 영상의 메타데이터를 변화시킨다
    - 문자 탐지(Text Detection)
        - 글자들이 존재하는 위치를 찾아내고 bounding box로 묶는다
        - 주로 CNN 계열의 모델 사용
    - 문자 인식(Text Recognition)
        - bounding box 안에 글자가 어떤 내용인지 알아낸다
        - CNN + RNN → CRNN 모델 사용
            - CNN을 통해 입력 이미지로부터 특성 시퀀스를 추출하고 이 특성 시퀀스들을 RNN의 입력값으로 하여 이미지의 텍스트 시퀀스를 예측 한다.
- 어려운 점
    - 문서 이미지 속 글자들은 정형화되어 있지만, 손글씨 및 서명은 비정형화되어 있어 분별이 어렵다
    - 배경이 복잡한 경우 배경과 문자의 구분이 힘들다. 노이즈, 왜곡, 글자 사이 밀도, 저해상도로 인해 식별에 어려움이 있음
    - 영문과 비교해 분류해야할 글자 수가 26→900개로 35배 정도 차이남

---

[AI-Hub](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100)

참조 : [https://velog.io/@freetix/리뷰-CRAFT-Character-Region-Awareness-for-Text-Detection](https://velog.io/@freetix/%EB%A6%AC%EB%B7%B0-CRAFT-Character-Region-Awareness-for-Text-Detection)

[https://medium.com/@msmapark2/character-region-awareness-for-text-detection-craft-paper-분석-da987b32609c](https://medium.com/@msmapark2/character-region-awareness-for-text-detection-craft-paper-%EB%B6%84%EC%84%9D-da987b32609c)

[Character Region Awareness for Text Detection.pdf](%E1%84%89%E1%85%A9%E1%86%AB%E1%84%80%E1%85%B3%E1%86%AF%E1%84%8A%E1%85%B5%20ocr%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20c323dfe423e249779dc4b5ffc695fb5d/Character_Region_Awareness_for_Text_Detection.pdf)

[대용량 손글씨 OCR 데이터]

- 텍스트 검출 모델(CRAFT 모델) - Naver Clova 팀에서 발표
    - CRAFT 알고리즘은 Scene Text Detection(STD) 문제를 해결하기 위해 기본적으로 딥러닝 기반 모델을 이용
    - 모델의 목적은 입력으로 사용한 이미지에 대해 ‘픽셀’마다 다음의 두 값을 예측하는 것
        - Region score : 해당 픽셀이 문자(character)의 중심일 확률
        - Affinity score : 해당 픽셀이 인접한 두 문장의 중심일 확률을 의미. 이 점수를 기반으로 개별 문자가 하나의 단어로 그룹화될 것인지 결정

![Untitled](%E1%84%89%E1%85%A9%E1%86%AB%E1%84%80%E1%85%B3%E1%86%AF%E1%84%8A%E1%85%B5%20ocr%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20c323dfe423e249779dc4b5ffc695fb5d/Untitled.png)

- [https://visionhong.tistory.com/31](https://visionhong.tistory.com/31)
    
    [Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf](%E1%84%89%E1%85%A9%E1%86%AB%E1%84%80%E1%85%B3%E1%86%AF%E1%84%8A%E1%85%B5%20ocr%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20c323dfe423e249779dc4b5ffc695fb5d/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf)
    
- 텍스트 인식 모델(swin - transformer)
    - vision transformer 중 한 모델

[Transformation for image recognition at scale.pdf](%E1%84%89%E1%85%A9%E1%86%AB%E1%84%80%E1%85%B3%E1%86%AF%E1%84%8A%E1%85%B5%20ocr%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20c323dfe423e249779dc4b5ffc695fb5d/Transformation_for_image_recognition_at_scale.pdf)

- vision transformer(비전 변환기) - Google research brain 팀에서 발표
    - NLP 분야에서 사용하고 있는 Transformer를 image classification 분야에 맞게 약간 변형하면서 기존의 CNN을 전혀 사용하지 않고 적용함
    - 단점으로는 데이터의 양이 많이 필요
        
        ![Untitled](%E1%84%89%E1%85%A9%E1%86%AB%E1%84%80%E1%85%B3%E1%86%AF%E1%84%8A%E1%85%B5%20ocr%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20c323dfe423e249779dc4b5ffc695fb5d/Untitled%201.png)
        
    
    Vision Transformer의 로직은 다음과 같다.
    
    1. 이미지를 여러개의 패치(base model의 patch는 16x16 크기)로 자른후에 각 패치별로 1차원 embedding demension(16x16x3 = 768)으로 만든다.
    2. class token이라는 것을 concatenate 시키고 각 패치마다 Position Embedding을 더해준다 (class token은 패치가 아닌 이미지 전체의 Embedding을 가지고 있다는 가정하에 최종 classification head에서 사용 / Position Embedding은 각 패치의 순서를 모델에 알려주는 역할을 한다) -> cls token과 positional embedding은 모두 학습되는 파라미터
    3. Transformer Encoder를 n번 수행을 한다. (base model은 12번의 block 수행) -> Layer Normalization을 사용하며 기존 바닐라 Transformer와는 다르게 Attention과 MLP이전에 수행을 하게 되면서 깊은 레이어에서도 학습이 잘 되도록 했다고 한다.
    4. 최종적으로 Linear연산을 통해 classification을 하게 된다.
    
    참조 : [https://visionhong.tistory.com/25#article-0-1--an-image-is-worth-16x16-words:-transformers-for-image-recognition-at-scale](https://visionhong.tistory.com/25#article-0-1--an-image-is-worth-16x16-words:-transformers-for-image-recognition-at-scale)
    
    - transformer는 Attention mechanism에 기반을 둔 모델이다
    
    [Attention Is All You Need.pdf](%E1%84%89%E1%85%A9%E1%86%AB%E1%84%80%E1%85%B3%E1%86%AF%E1%84%8A%E1%85%B5%20ocr%20%E1%84%86%E1%85%A9%E1%84%83%E1%85%A6%E1%86%AF%20c323dfe423e249779dc4b5ffc695fb5d/Attention_Is_All_You_Need.pdf)
    
    - Attention mechanism은 seq2seq 모델(번역, 요약과 같이 시퀀스(sequence)를 입력받아 시퀀스를 출력하는 task를 위해 고안된 RNN 기반 모델)의 문제점을 개선하기 위해 제안되었다