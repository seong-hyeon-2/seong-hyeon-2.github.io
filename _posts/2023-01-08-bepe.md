---
layout: post
title:  "비즈니스 프로그래밍 형태소 분석 보고서"
---

# 비즈니스 프로그래밍 기말 최종 보고서

201921429 경영학과 강성현

===================================================================================================================

# 프로젝트 주제 

대략 1~2달 동안 패션 잡지 3사(VOGUE, W, ELLE)에서 게시한 글을 스크랩하고 키워드를 추출해 현재 패션 트렌드 알아보기

===================================================================================================================

## 주제 선정 이유

 패션에 정답이 없다고는 하지만 패션 트렌드를 어느 정도는 파악하고 자신만의 스타일에 조금씩 녹이는게 촌스럽지도 않고 자신의 개성을 계속해서 이어나갈 수 있다고 생각한다. 이런 트렌드를 쉽게 알 수 있다면, 쇼핑을 하거나 코디를 할 때 도움을 줄 것이라 생각한다. 그래서 이번 프로젝트를 통해 패션 잡지 3곳(VOGUE, W, ELLE)의 게시물을 크롤링해보고 키워드를 살펴보면 현재 또는 다가올 패션 트렌드를 파악하는데 조금의 도움을 줄 것이라 생각해 이 주제를 선정하였다.

## 자료 수집 및 활용 방안

VOGUE korea

https://www.vogue.co.kr/category/fashion/

W

https://www.wkorea.com/category/fashion/

ELLE korea

https://www.elle.co.kr/fashion

  위 잡지 3사 링크에 들어가서 대략 1~2달 게시글을 크롤링해 키워드를 뽑을 것이다. 키워드 추출 시 형태소 분석 라이브러리인 kiwi를 사용할 것이며, 형태소를 추출하는 과정에서 일반 명사(NNG), 고유 명사(NNP)를 각각 추출할 것이다. 일반 명사와 고유 명사를 같이 추출하게 되면, 일반 명사가 너무 많이 추출되어 키워드를 잘 추출할 수 없다고 판단하였다. 
 
  이렇게 추출한 일반 명사와 고유 명사 키워드들을 각 잡지마다, 세 잡지사 키워드를 모두 합쳐 matplotlib, wordcloud,squarify를 이용해 시각화을 진행하여, 요즘 패션 트렌드가 무엇인지 분석해볼 예정이다.

# 크롤링 전 사전 작업


```python
# 패키지 설치
# 맥북 m1에서는 kiwipiepy만 설치하면 에러가 나기 때문에
# cmake도 같이 설치해줘야함.
# 출처 : https://github.com/bab2min/kiwipiepy

! pip install cmake
! pip install --upgrade pip
! pip install kiwipiepy
```

    Requirement already satisfied: cmake in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (3.25.0)
    Requirement already satisfied: pip in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (22.3.1)
    Requirement already satisfied: kiwipiepy in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (0.14.0)
    Requirement already satisfied: kiwipiepy-model~=0.14 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from kiwipiepy) (0.14.0)
    Requirement already satisfied: dataclasses in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from kiwipiepy) (0.6)



```python
!pip install matplotlib
```

    Requirement already satisfied: matplotlib in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (3.5.2)
    Requirement already satisfied: kiwisolver>=1.0.1 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.4.2)
    Requirement already satisfied: numpy>=1.17 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.21.5)
    Requirement already satisfied: pillow>=6.2.0 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (9.2.0)
    Requirement already satisfied: pyparsing>=2.2.1 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.0.9)
    Requirement already satisfied: packaging>=20.0 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (21.3)
    Requirement already satisfied: python-dateutil>=2.7 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (2.8.2)
    Requirement already satisfied: cycler>=0.10 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.11.0)
    Requirement already satisfied: fonttools>=4.22.0 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (4.25.0)
    Requirement already satisfied: six>=1.5 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)



```python
!pip install wordcloud
```

    Collecting wordcloud
      Downloading wordcloud-1.8.2.2.tar.gz (220 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m220.8/220.8 kB[0m [31m7.2 MB/s[0m eta [36m0:00:00[0m
    [?25h  Preparing metadata (setup.py) ... [?25ldone
    [?25hRequirement already satisfied: numpy>=1.6.1 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (1.21.5)
    Requirement already satisfied: pillow in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (9.2.0)
    Requirement already satisfied: matplotlib in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (3.5.2)
    Requirement already satisfied: pyparsing>=2.2.1 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (3.0.9)
    Requirement already satisfied: python-dateutil>=2.7 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (2.8.2)
    Requirement already satisfied: fonttools>=4.22.0 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (4.25.0)
    Requirement already satisfied: packaging>=20.0 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (21.3)
    Requirement already satisfied: cycler>=0.10 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (0.11.0)
    Requirement already satisfied: kiwisolver>=1.0.1 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (1.4.2)
    Requirement already satisfied: six>=1.5 in /Users/seong_h/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)
    Building wheels for collected packages: wordcloud
      Building wheel for wordcloud (setup.py) ... [?25ldone
    [?25h  Created wheel for wordcloud: filename=wordcloud-1.8.2.2-cp39-cp39-macosx_11_0_arm64.whl size=152816 sha256=eae5f8eee01f546aaf0505e50467bb84d250011c5a7432ebfac4e98005386348
      Stored in directory: /Users/seong_h/Library/Caches/pip/wheels/80/53/9d/ac9f8de67897f4804f21caa5a42ed7afb86669f17a0b241bc1
    Successfully built wordcloud
    Installing collected packages: wordcloud
    Successfully installed wordcloud-1.8.2.2



```python
!pip install squarify
```

    Collecting squarify
      Downloading squarify-0.4.3-py3-none-any.whl (4.3 kB)
    Installing collected packages: squarify
    Successfully installed squarify-0.4.3



```python
#selenium을 불러오고 webdriver을 활용해 웹 크롤링을 하는 기능
from selenium import webdriver 

#URL을 통해 데이터를 가져오기 위한 모듈
import requests

#HTML 데이터를 처리해주는 모듈
from bs4 import BeautifulSoup

#구조화된 데이터 형식 저장을 위한 모듈
import json 

#정규표현식 사용을 위한 모듈
import re

#JSON의 문자열 구문분석을 위한 모듈
import ujson 

#각 url 사이에 delay를 넣기 위한 모듈
import time 

# 집합을 생성해주는 모듈
from ordered_set import OrderedSet

# 셀레니움이 바뀌어서 새로운 모듈을 설치해야함.
from selenium.webdriver.common.by import By

from selenium.webdriver.common.keys import Keys
```


```python
# 세 잡자사 URL
VOGUE_URL = 'https://www.vogue.co.kr/category/fashion/'
W_URL = f'https://www.wkorea.com/category/fashion'
ELLE_URL = 'https://www.elle.co.kr/fashion/streetstyle'
```


```python
# 크롤링하는 주체가 로봇이 아님을 나타냄. 

USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15"
HEADERS = {"User-Agent": USER_AGENT}
```


```python
# chromedriver를 가져온다.
# 크롬 창이 열린다.

driver = webdriver.Chrome('/Users/seong_h/Desktop/학교/crawring/chromedriver')
```

    /var/folders/hy/54qrqgsx1jb1jxv2kwz5b_0h0000gn/T/ipykernel_45896/2467853471.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object
      driver = webdriver.Chrome('/Users/seong_h/Desktop/학교/crawring/chromedriver')


# 잡지 3사 - URL 가져오기 

제목과 본문 키워드를 파악하기 전에 크롤링할 게시물의 url을 가져와야한다. 


```python
# vogue url에 접근한다. 
driver.get(VOGUE_URL)

# vogue 페이지는 스크롤을 내려야 더 많은 글을 볼 수 있음. --> 1페이지 넘어가는 것과 같음
# Keys.PAGE_DOWN를 통해 계속해서 스크롤을 내림
count = 0

while count < 150:
    driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.PAGE_DOWN)
    count += 1
    if count % 10 == 0:
        # 대략 스크롤 10번 내리면 더 많은 글을 보기위해 로딩이 걸림. 
        # 그 시간 맞춰 time.sleep을 해주면 효율적으로 스크롤 내릴 수 있음
        time.sleep(2)

# 게시물 url 들어갈 집합 만들기
vogue_post_url = OrderedSet()

html = driver.page_source
soup = BeautifulSoup(html, 'lxml')

urls = soup.select("div.fusion-post-wrapper > a")

# 게시물 주소 가져오기
for url in urls: 
    vogue_post_url.append(url.attrs["href"])
    
print(len((vogue_post_url)))
```

    228



```python
# W url 들어갈 집합 만들기
w_post_url = OrderedSet()

# W는 링크 뒤에 페이지 수가 적혀있다.
# 반복문을 이용해 한 페이지마다 들어가서 링크를 가져온다.
for num in range(1, 24):
    # url에 접근한다. 
    driver.get(W_URL + f'/page/{num}/')
    html = driver.page_source
    soup = BeautifulSoup(html, 'lxml')
    
    urls = soup.find_all("h2", {'class': 'entry-title fusion-post-title'})
    
    # url 가져오기
    for url in urls:
        url = url.find("a")["href"]
        w_post_url.append(url)
        
print(len(w_post_url))
```

    230



```python
# ELLE url 들어갈 집합 만들기
elle_post_url = OrderedSet()

# url에 접근한다. 
driver.get(ELLE_URL)

# ELLE는 맨 밑에 load more을 눌러줘야 더 많은 글을 볼 수 있다.
# 그래서 15번 정도 load more을 클릭해주면 230개 정도의 글의 url을 긁어올 수 있다.
count = 0
while count < 16:
    # Load More 클릭 
    load_more = driver.find_element(By.XPATH, "//div[@class='btn_box_more']")
    load_more.click()
    time.sleep(2)
    count += 1


html = driver.page_source
soup = BeautifulSoup(html, 'lxml')

urls = soup.find_all("em", {'class': 'title'})

# ELLE의 url은 특이하게 수집한 url이 뒤쪽 부분만 나와있어 url을 수집할때 
# https://www.elle.co.kr/ 이거와 붙여서 넣어줘야한다.
for url in urls:
    url = url.find("a")["href"]
    elle_post_url.append('https://www.elle.co.kr/'+ url)
    
print(len(elle_post_url))

```

    234


#  잡지 3사 제목 및 본문 가져오기

제목과 본문의 키워드를 파악하기 전에 먼저 각각의 리스트 안에 제목과 본문을 넣는 작업을 거친다.


```python
# vogue 게시물 제목 및 본문 하나씩 크롤링하기

# vogue 게시물 제목과 내용이 들어갈 리스트
vogue_title_list = []
vogue_content_list = []

# enumerate를 이용해 index와 url을 같이 가지고온다.
for idx, url in enumerate(vogue_post_url): 
    if idx % 5 == 0:
        print(f" {idx+1}/{len(vogue_post_url)} ")
    
    # 각 게시물 하나씩 들어가기
    driver.get(url)
    time.sleep(1)
    html = driver.page_source
    soup = BeautifulSoup(html, 'lxml')

    # 제목 가져오기 
    title = soup.find('h2', class_ = 'entry-title fusion-post-title').text
    vogue_title_list.append(title)

    # 본문 가져오기
    contents = soup.find_all('div', class_ = 'post-content')
    
    # 본문 긁어온거에서 text만 가져옴
    for content in contents:
        content = content.text
        # 리스트에 text만 넣음
        vogue_content_list.append(content)
```

     1/228 
     6/228 
     11/228 
     16/228 
     21/228 
     26/228 
     31/228 
     36/228 
     41/228 
     46/228 
     51/228 
     56/228 
     61/228 
     66/228 
     71/228 
     76/228 
     81/228 
     86/228 
     91/228 
     96/228 
     101/228 
     106/228 
     111/228 
     116/228 
     121/228 
     126/228 
     131/228 
     136/228 
     141/228 
     146/228 
     151/228 
     156/228 
     161/228 
     166/228 
     171/228 
     176/228 
     181/228 
     186/228 
     191/228 
     196/228 
     201/228 
     206/228 
     211/228 
     216/228 
     221/228 
     226/228 



```python
# W 게시물 제목 및 본문 하나씩 크롤링하기

# W 게시물 제목과 내용이 들어갈 리스트
w_title_list = []
w_content_list = []

for idx, url in enumerate(w_post_url): 
    if idx % 5 == 0:
        print(f" {idx+1}/{len(w_post_url)} ")
    
    # 각 게시물 하나씩 들어가기
    driver.get(url)
    time.sleep(1)
    html = driver.page_source
    soup = BeautifulSoup(html, 'lxml')
    
    
    # 제목 가져오기 
    title = soup.find('h2', class_ = 'entry-title fusion-post-title').text
    w_title_list.append(title)
    
        
    # 본문 가져오기
    contents = soup.find_all('div', class_ = 'post-content')
    
    
    # 본문 긁어온거에서 text만 가져옴
    for content in contents:
        content = content.text
        w_content_list.append(content)
```

     1/230 
     6/230 
     11/230 
     16/230 
     21/230 
     26/230 
     31/230 
     36/230 
     41/230 
     46/230 
     51/230 
     56/230 
     61/230 
     66/230 
     71/230 
     76/230 
     81/230 
     86/230 
     91/230 
     96/230 
     101/230 
     106/230 
     111/230 
     116/230 
     121/230 
     126/230 
     131/230 
     136/230 
     141/230 
     146/230 
     151/230 
     156/230 
     161/230 
     166/230 
     171/230 
     176/230 
     181/230 
     186/230 
     191/230 
     196/230 
     201/230 
     206/230 
     211/230 
     216/230 
     221/230 
     226/230 



```python
# ELLE 게시물 제목과 내용이 들어갈 리스트

# ELLE 게시물 제목과 내용이 들어갈 리스트
elle_title_list = []
elle_content_list = []

for idx, url in enumerate(elle_post_url): 
    if idx % 5 == 0:
        print(f" {idx+1}/{len(elle_post_url)} ")
    
    # 각 게시물 하나씩 들어가기
    driver.get(url)
    time.sleep(1)
    html = driver.page_source
    soup = BeautifulSoup(html, 'lxml')

    
    # 제목 가져오기 
    title = soup.find('h2', class_ = 'tit_article').text
    elle_title_list.append(title)

        
    # 본문 가져오기
    contents = soup.find_all('div', class_ = 'atc_content')

    # 본문 긁어온거에서 text만 가져옴
    for content in contents:
        content = content.text
        elle_content_list.append(content)
```

     1/234 
     6/234 
     11/234 
     16/234 
     21/234 
     26/234 
     31/234 
     36/234 
     41/234 
     46/234 
     51/234 
     56/234 
     61/234 
     66/234 
     71/234 
     76/234 
     81/234 
     86/234 
     91/234 
     96/234 
     101/234 
     106/234 
     111/234 
     116/234 
     121/234 
     126/234 
     131/234 
     136/234 
     141/234 
     146/234 
     151/234 
     156/234 
     161/234 
     166/234 
     171/234 
     176/234 
     181/234 
     186/234 
     191/234 
     196/234 
     201/234 
     206/234 
     211/234 
     216/234 
     221/234 
     226/234 
     231/234 


# kiwi import 

한국어 형태소를 제일 잘 분류할 수 있는 모듈이기 때문에, 이번 프로젝트에서 활용한다.


```python
# kiwi 패키지 import 
from kiwipiepy import Kiwi 
kiwi = Kiwi(model_type='sbg')

# 일반 명사와 고유 명사를 따로 추출할 것이기 때문에, 따로 설정한다.

CHK_POS_NNG = {"NNG", # 일반 명사
           # "NNP", # 고유 명사
           # "XR", # 어근
           # "NP" # 대명사 
}

CHK_POS_NNP = {# "NNG", # 일반 명사
           "NNP", # 고유 명사
           # "XR", # 어근
           # "NP" # 대명사 
}
```

# 잡지 3사 제목 및 본문에서 일반 명사 빈도수 

먼저 잡지 3사의 제목 및 본문에서 일반 명사의 빈도수를 파악하도록 하겠다. 
그후 모든 잡지사의 키워드를 합쳐 빈도수가 높은 키워드 50개를 추출한다.

## 잡지 3사 본문에서 일반 명사 빈도수


```python
# vogue 본문에서 일반 명사 추출 


# vogue 게시물 본문에서 일반 명사가 들어갈 리스트
vogue_content_nng = []

for content in vogue_content_list:
    for token in kiwi.tokenize(content):
        if tuple(token)[1] in CHK_POS_NNG:
            # token 튜플에서 키워드만 리스트에 추가한다.
            vogue_content_nng.append(tuple(token)[0])

# 키워드의 빈도 수를 파악한다. 
from collections import Counter
vogue_content_nng_counter = Counter(vogue_content_nng)
# 빈도수 상위 50개 키워드를 추출한다.
vogue_content_nng_most = vogue_content_nng_counter.most_common(50)



# 불용어 처리 
for i in vogue_content_nng_most:    
    if i[0] in ['이전','다음','공식','스타일','사이즈', '홈페이지',
                '톱', '룩', '장식', '모습', '제품','말','때','선택',
                '컬렉션','아이템','브랜드', '스타일링', '완성', '소재', 
                '연출','이번','사람', '사랑','마무리', '미니']:
        # 위 리스트에 들어간 불용어가 있으면 제거
        vogue_content_nng_most.remove(i)

# 딕셔너리 생성  
# 밑 워드클라우드를 사용하기 위해 딕셔너리를 생성한다.
vogue_content_nng_dict = {}

for i in vogue_content_nng_most:
    vogue_content_nng_dict[i[0]] = i[1]
    
print(vogue_content_nng_most)
```

    [('컬러', 428), ('패션', 425), ('부츠', 319), ('아이템', 283), ('재킷', 248), ('매치', 231), ('스타일링', 220), ('드레스', 219), ('팬츠', 209), ('코트', 192), ('데님', 185), ('디자인', 161), ('실루엣', 160), ('활용', 159), ('스커트', 154), ('블랙', 140), ('무드', 140), ('겨울', 132), ('느낌', 129), ('연출', 120), ('디테일', 118), ('착용', 115), ('다양', 115), ('사이즈', 114), ('톱', 111), ('수트', 110), ('디자이너', 108), ('시작', 107), ('정도', 107), ('트렌드', 107), ('모델', 107), ('포인트', 106), ('슈즈', 106), ('라인', 104), ('생각', 102), ('매력', 99), ('셔츠', 98)]



```python
# w 본문에서 일반 명사 추출 

# W 게시물 본문에서 일반 명사가 들어갈 리스트
w_content_nng = []

for content in w_content_list:
    for token in kiwi.tokenize(content):
        if tuple(token)[1] in CHK_POS_NNG:
            # token 튜플에서 키워드만 리스트에 추가한다.
            w_content_nng.append(tuple(token)[0])

# 키워드 개수 세기
from collections import Counter
w_content_nng_counter = Counter(w_content_nng)
# 빈도수 상위 50개 키워드를 추출한다.
w_content_nng_most = w_content_nng_counter.most_common(50)


# 불용어 처리
for i in w_content_nng_most:    
    if i[0] in ['이전','다음','공식','스타일','사이즈', '홈페이지',
                '톱', '룩', '장식', '모습', '제품','말','때','선택',
                '컬렉션','아이템','브랜드', '스타일링', '완성', '소재', 
                '연출','이번','사람', '사랑','마무리', '미니']:
        w_content_nng_most.remove(i)

# 딕셔너리 생성     
# 밑 워드클라우드를 사용하기 위해 딕셔너리를 생성한다.
w_content_nng_dict = {}
   
for i in w_content_nng_most:
    w_content_nng_dict[i[0]] = i[1]
    
print(w_content_nng_most)
```

    [('룩', 383), ('패션', 348), ('컬러', 290), ('드레스', 257), ('재킷', 255), ('팬츠', 254), ('골드', 230), ('다이아몬드', 200), ('부츠', 186), ('스커트', 174), ('매치', 169), ('디자인', 161), ('착용', 151), ('세팅', 147), ('모습', 136), ('모델', 133), ('쇼', 131), ('링', 129), ('니트', 126), ('컷', 125), ('코트', 123), ('다양', 116), ('사람', 114), ('데님', 111), ('생각', 108), ('워치', 108), ('블랙', 106), ('가죽', 106), ('셔츠', 105), ('포인트', 105), ('슈즈', 104), ('활용', 104), ('옷', 102), ('시즌', 99), ('매력', 99), ('로고', 99), ('백', 98)]



```python
# elle 본문에서 일반 명사 추출 
 
# elle 게시물 본문에서 일반 명사가 들어갈 리스트
elle_content_nng = []

for content in elle_content_list:
    for token in kiwi.tokenize(content):
        if tuple(token)[1] in CHK_POS_NNG:
            # token 튜플에서 키워드만 리스트에 추가한다.
            elle_content_nng.append(tuple(token)[0])
        
# 키워드 개수 세기
from collections import Counter
elle_content_nng_counter = Counter(elle_content_nng)
# 빈도수 상위 50개 키워드를 추출한다.
elle_content_nng_most = elle_content_nng_counter.most_common(50)


# 불용어 처리
for i in elle_content_nng_most:    
    if i[0] in ['이전','다음','공식','스타일','사이즈', '홈페이지',
                '톱', '룩', '장식', '모습', '제품','말','때','선택',
                '컬렉션','아이템','브랜드', '스타일링', '완성', '소재', 
                '연출','이번','사람', '사랑','마무리', '미니']:
        elle_content_nng_most.remove(i)
      
    
# 딕셔너리 생성     
# 밑 워드클라우드를 사용하기 위해 딕셔너리를 생성한다.
elle_content_nng_dict = {}

for i in elle_content_nng_most:
    elle_content_nng_dict[i[0]] = i[1]
    
print(elle_content_nng_most)
```

    [('컬러', 727), ('이전', 474), ('매치', 445), ('패션', 350), ('화이트', 316), ('재킷', 295), ('팬츠', 289), ('포인트', 271), ('패턴', 261), ('공식', 240), ('블랙', 237), ('데님', 222), ('드레스', 222), ('스타일링', 219), ('분위기', 218), ('무드', 214), ('셔츠', 209), ('디자인', 207), ('디테일', 205), ('소재', 187), ('매력', 158), ('티셔츠', 146), ('니트', 145), ('핑크', 145), ('헤어', 145), ('활용', 144), ('선글라스', 139), ('스커트', 135), ('톤', 128), ('사랑', 123), ('클래식', 123), ('라인', 112), ('부츠', 112), ('모델', 111), ('백', 111), ('마무리', 99), ('원피스', 98), ('블루', 92)]



```python
# 잡지 3사 본문에서 일반 명사 빈도수 

# 잡지 3사 키워드 리스트를 한 리스트로 만듦
sum_content_nng = vogue_content_nng + w_content_nng + elle_content_nng

sum_content_nng_list = []
sum_content_nng_dict = {}

# 키워드 개수 세기
from collections import Counter
sum_content_nng_counter = Counter(sum_content_nng)
# 빈도수 상위 50개 키워드를 추출한다.
sum_content_nng_most = sum_content_nng_counter.most_common(50)



# 불용어 처리
for i in sum_content_nng_most:    
    if i[0] in ['톱', '골드', '공식', '완성', '포인트', '때', '톱', '활용',
               '말', '모습', '컬렉션', '장식', '사랑', '매력', '사이즈', '이번',
               '다음', '룩', '다양', '소재', '매치', '제품']:
        sum_content_nng_most.remove(i)

# 딕셔너리 생성        
for i in sum_content_nng_most:
    sum_content_nng_dict[i[0]] = i[1]



print('3사 잡지사 본문 고유 명사: \n',sum_content_nng_most)
print()
print('3사 잡지사 본문 고유 명사: \n',sum_content_nng_dict)
```

    3사 잡지사 본문 고유 명사: 
     [('컬러', 1445), ('패션', 1123), ('제품', 808), ('재킷', 798), ('스타일', 774), ('팬츠', 752), ('드레스', 698), ('아이템', 632), ('스타일링', 619), ('부츠', 617), ('선택', 531), ('디자인', 529), ('완성', 519), ('데님', 518), ('이전', 496), ('화이트', 488), ('블랙', 483), ('장식', 473), ('스커트', 463), ('무드', 444), ('연출', 443), ('브랜드', 440), ('패턴', 426), ('셔츠', 412), ('디테일', 395), ('코트', 382), ('니트', 360), ('사랑', 354), ('모델', 351), ('분위기', 339), ('실루엣', 315), ('공식', 301), ('착용', 297), ('슈즈', 282)]
    
    3사 잡지사 본문 고유 명사: 
     {'컬러': 1445, '패션': 1123, '제품': 808, '재킷': 798, '스타일': 774, '팬츠': 752, '드레스': 698, '아이템': 632, '스타일링': 619, '부츠': 617, '선택': 531, '디자인': 529, '완성': 519, '데님': 518, '이전': 496, '화이트': 488, '블랙': 483, '장식': 473, '스커트': 463, '무드': 444, '연출': 443, '브랜드': 440, '패턴': 426, '셔츠': 412, '디테일': 395, '코트': 382, '니트': 360, '사랑': 354, '모델': 351, '분위기': 339, '실루엣': 315, '공식': 301, '착용': 297, '슈즈': 282}


## 잡지 3사 제목에서 일반 명사 빈도수


```python
# vogue 제목에서 일반 명사 추출 및 빈도수

# 제목 키워드 들어갈 리스트 
vogue_title_nng = []

for title in vogue_title_list:
    for token in kiwi.tokenize(title):
        # 키워드 추출하기 
        if tuple(token)[1] in CHK_POS_NNG:
            vogue_title_nng.append(tuple(token)[0])

# 키워드 개수 세기
from collections import Counter
vogue_title_nng_counter = Counter(vogue_title_nng)
# 상위 20개 키워드 추출
vogue_title_nng_most = vogue_title_nng_counter.most_common(20)
print(vogue_title_nng_most)
```

    [('패션', 30), ('겨울', 20), ('스타일링', 17), ('룩', 15), ('부츠', 11), ('트렌드', 10), ('선택', 9), ('패딩', 9), ('스타', 8), ('코트', 8), ('컬렉션', 7), ('브랜드', 7), ('멋', 6), ('유행', 6), ('요즘', 6), ('스타일', 6), ('주', 6), ('슈즈', 5), ('때', 5), ('애정', 5)]



```python
# w 제목에서 일반 명사 추출 및 빈도수

# 제목 키워드 들어갈 리스트 
w_title_nng = []

for title in w_title_list:
    for token in kiwi.tokenize(title):
        # 키워드 추출하기 
        if tuple(token)[1] in CHK_POS_NNG:
            w_title_nng.append(tuple(token)[0])
                
# 키워드 개수 세기
from collections import Counter
w_title_nng_counter = Counter(w_title_nng)
# 상위 20개 키워드 추출
w_title_nng_most = w_title_nng_counter.most_common(20)

print(w_title_nng_most)
```

    [('패션', 24), ('화보', 15), ('룩', 15), ('컬렉션', 11), ('크리스마스', 9), ('공개', 8), ('선', 7), ('겨울', 7), ('속', 7), ('가을', 7), ('핑크', 6), ('트렌드', 6), ('셀럽', 5), ('블랙', 5), ('코트', 5), ('커버', 5), ('신상', 5), ('스타일', 5), ('위크', 5), ('재킷', 5)]



```python
# elle 제목에서 일반 명사 추출 및 빈도수

elle_title_nng = []

for title in elle_title_list:
    for token in kiwi.tokenize(title):
        # 키워드 추출하기
        if tuple(token)[1] in CHK_POS_NNG:
            elle_title_nng.append(tuple(token)[0])

# 키워드 개수 세기
from collections import Counter
elle_title_nng_counter = Counter(elle_title_nng)
# 상위 20개 키워드 추출
elle_title_nng_most = elle_title_nng_counter.most_common(20)
print(elle_title_nng_most)
```

    [('스타', 42), ('패션', 37), ('룩', 29), ('스타일링', 25), ('여름', 21), ('쇼핑', 18), ('스타일', 15), ('리스트', 14), ('추천', 11), ('겨울', 10), ('재킷', 10), ('가을', 9), ('트렌드', 8), ('인', 8), ('매력', 8), ('활용법', 8), ('봄', 8), ('백', 7), ('힙', 7), ('사랑', 6)]



```python
# 모든 잡지사 제목에서 일반 명사 빈도수 

# 잡지 3사 제목 키워드 한 리스트로 만들기
sum_title_nng = vogue_title_nng + w_title_nng + elle_title_nng

sum_title_nng_list = []
sum_title_nng_dict = {}

# 키워드 개수 세기
from collections import Counter
sum_title_nng_counter = Counter(sum_title_nng)
# 상위 키워드 20개 추출
sum_title_nng_most = sum_title_nng_counter.most_common(20)

# 딕셔너리 만들기
# wordcloud에 쓰기 위해 딕셔너리 생성
for i in sum_title_nng_most:
    tup = (i[0], i[1])
    sum_title_nng_list.append(tup)
    sum_title_nng_dict[i[0]] = i[1]
    

print('3사 잡지사 제목 일반 명사: \n',sum_title_nng_list)
print('3사 잡지사 제목 일반 명사: \n',sum_title_nng_dict)
```

    3사 잡지사 제목 일반 명사: 
     [('패션', 91), ('룩', 59), ('스타', 51), ('스타일링', 43), ('겨울', 37), ('스타일', 26), ('트렌드', 24), ('여름', 23), ('컬렉션', 19), ('리스트', 19), ('쇼핑', 18), ('부츠', 17), ('재킷', 17), ('가을', 17), ('코트', 16), ('속', 16), ('화보', 16), ('패딩', 15), ('크리스마스', 15), ('사랑', 15)]
    3사 잡지사 제목 일반 명사: 
     {'패션': 91, '룩': 59, '스타': 51, '스타일링': 43, '겨울': 37, '스타일': 26, '트렌드': 24, '여름': 23, '컬렉션': 19, '리스트': 19, '쇼핑': 18, '부츠': 17, '재킷': 17, '가을': 17, '코트': 16, '속': 16, '화보': 16, '패딩': 15, '크리스마스': 15, '사랑': 15}


# 잡지 3사에서 제목 및 본문 고유 명사 빈도수

## 잡지 3사 제목에서 고유 명사 빈도수


```python
# vogue 제목에서 고유 명사 추출 및 빈도수

# 제목 키워드 들어갈 리스트 
vogue_title_nnp = []

for title in vogue_title_list:
    for token in kiwi.tokenize(title):
        # 키워드 추출하기
        if tuple(token)[1] in CHK_POS_NNP:
            vogue_title_nnp.append(tuple(token)[0])

# 키워드 개수 세기
from collections import Counter
vogue_title_nnp_counter = Counter(vogue_title_nnp)
# 키워드 상위 20개 추출
vogue_title_nnp_most = vogue_title_nnp_counter.most_common(20)

print(vogue_title_nnp_most)
```

    [('비버', 6), ('인스타그램', 6), ('헤일리', 4), ('샤넬', 4), ('에밀리', 4), ('켄달', 4), ('제너', 4), ('롱', 3), ('하디드', 3), ('수트', 3), ('저스틴', 3), ('알레산드로', 3), ('미켈레', 3), ('방탄소년단', 3), ('서울', 2), ('제니', 2), ('케이트', 2), ('비니', 2), ('프라다', 2), ('셀럽', 2)]



```python
# w 제목에서 고유 명사 추출 및 빈도수

# 제목 키워드 들어갈 리스트 
w_title_nnp = []

for title in w_title_list:
    for token in kiwi.tokenize(title):
        # 키워드 추출하기
        if tuple(token)[1] in CHK_POS_NNP:
            w_title_nnp.append(tuple(token)[0])

# 키워드 개수 세기
from collections import Counter
w_title_nnp_counter = Counter(w_title_nnp)
# 키워드 상위 20개 추출
w_title_nnp_most = w_title_nnp_counter.most_common(20)

print(w_title_nnp_most)
```

    [('샤넬', 11), ('뉴진스', 10), ('더블유', 8), ('제주', 4), ('셧', 4), ('다운', 4), ('제니', 3), ('홀리데이', 3), ('소희', 3), ('알렉산더', 3), ('구찌', 3), ('파리', 3), ('장원영', 3), ('맥퀸', 2), ('발렌시아가', 2), ('밀란', 2), ('박보검', 2), ('주얼리', 2), ('바이브', 2), ('몬스타엑스', 2)]



```python
# elle 제목에서 고유 명사 추출 및 빈도수

# 제목 키워드 들어갈 리스트 
elle_title_nnp = []

for title in elle_title_list:
    for token in kiwi.tokenize(title):
        # 키워드 추출하기
        if tuple(token)[1] in CHK_POS_NNP:
            elle_title_nnp.append(tuple(token)[0])

# 키워드 개수 세기
from collections import Counter
elle_title_nnp_counter = Counter(elle_title_nnp)
# 상위 키워드 20개 추출
elle_title_nnp_most = elle_title_nnp_counter.most_common(20)

print(elle_title_nnp_most)
```

    [('엘르', 5), ('샤넬', 5), ('김나영', 5), ('김고은', 4), ('제니', 3), ('스니커즈', 3), ('벨라', 3), ('하디드', 3), ('치트키', 3), ('김세정', 3), ('김태리', 2), ('최', 2), ('강민경', 2), ('파리', 2), ('스트라이프', 2), ('시크', 2), ('박은빈', 2), ('러버', 2), ('타투', 2), ('핑크', 2)]



```python
# 잡지 3사 제목에서 고유 명사 빈도수

# 모든 잡지 키워드 한 리스트로 만들기
sum_title_nnp = vogue_title_nnp + w_title_nnp + elle_title_nnp

sum_title_nnp_dict = {}

# 키워드 개수 세기
from collections import Counter
sum_title_nnp_counter = Counter(sum_title_nnp)
# 상위 키워드 20개 추출
sum_title_nnp_most = sum_title_nnp_counter.most_common(20)


# 딕셔너리 생성        
for i in sum_title_nnp_most:
    sum_title_nnp_dict[i[0]] = i[1]



print('3사 잡지사 본문 고유 명사: \n',sum_title_nnp_most)
print()
print('3사 잡지사 본문 고유 명사: \n',sum_title_nnp_dict)
```

    3사 잡지사 본문 고유 명사: 
     [('샤넬', 20), ('뉴진스', 11), ('제니', 8), ('더블유', 8), ('비버', 7), ('하디드', 7), ('인스타그램', 6), ('켄달', 6), ('제너', 6), ('홀리데이', 5), ('에밀리', 5), ('제주', 5), ('파리', 5), ('김고은', 5), ('엘르', 5), ('김나영', 5), ('서울', 4), ('헤일리', 4), ('저스틴', 4), ('런웨이', 4)]
    
    3사 잡지사 본문 고유 명사: 
     {'샤넬': 20, '뉴진스': 11, '제니': 8, '더블유': 8, '비버': 7, '하디드': 7, '인스타그램': 6, '켄달': 6, '제너': 6, '홀리데이': 5, '에밀리': 5, '제주': 5, '파리': 5, '김고은': 5, '엘르': 5, '김나영': 5, '서울': 4, '헤일리': 4, '저스틴': 4, '런웨이': 4}


##  잡지 3사 본문에서 고유 명사 빈도수


```python
# vogue 본문에서 고유 명사 빈도수

# 본문에서 추출한 키워드가 들어갈 리스트 
vogue_content_nnp = []

for content in vogue_content_list:        
    for token in kiwi.tokenize(content):
        # 키워드 추출 
        if tuple(token)[1] in CHK_POS_NNP:
            vogue_content_nnp.append(tuple(token)[0])

# 키워드 개수 세기
from collections import Counter
vogue_content_nnp_counter = Counter(vogue_content_nnp)
# 상위 키워드 50개 추출 
vogue_content_nnp_most = vogue_content_nnp_counter.most_common(50)

# 불용어 처리
for i in vogue_content_nnp_most:    
    if i[0] in ['2','3','4','드','햇','킴', '주', '리']:
        vogue_content_nnp_most.remove(i)
        
vogue_content_nnp_dict = {}

# 딕셔너리 생성 
# wordcloud 만들기 위해서 딕셔너리 생성
for i in vogue_content_nnp_most:
    vogue_content_nnp_dict[i[0]] = i[1]
    
print(vogue_content_nnp_most)
```

    [('샤넬', 81), ('구찌', 77), ('런웨이', 73), ('레더', 69), ('프라다', 69), ('뉴욕', 56), ('비버', 55), ('하디드', 52), ('블레이저', 52), ('에밀리', 52), ('블랙', 51), ('헤일리', 50), ('벨라', 42), ('보그', 41), ('시크', 41), ('인스타그램', 39), ('미국', 38), ('테일러', 36), ('롱', 36), ('케이트', 34), ('백', 33), ('엘라', 32), ('마르지', 31), ('로랑', 31), ('백남준', 31), ('라프', 29), ('디올', 28), ('비니', 28), ('로에베', 27), ('아디다스', 27), ('제나', 27), ('조던', 26), ('시몬스', 26), ('발라클라바', 25), ('켄달', 25), ('제너', 25), ('레깅스', 24), ('루이', 24), ('파리', 23), ('저스틴', 23), ('한국', 22), ('텍스처', 22), ('힐', 22), ('발렌시아가', 22), ('마르탱', 21), ('어그', 21)]



```python
# w 본문에서 고유 명사 빈도수

# 본문에서 추출한 키워드가 들어갈 리스트 
w_content_nnp = []

for content in w_content_list:
    for token in kiwi.tokenize(content):
        # 키워드 추출 
        if tuple(token)[1] in CHK_POS_NNP:
            w_content_nnp.append(tuple(token)[0])

# 키워드 개수 세기
from collections import Counter
w_content_nnp_counter = Counter(w_content_nnp)
# 상위 키워드 50개 추출 
w_content_nnp_most = w_content_nnp_counter.most_common(50)

# 불용어 처리
for i in w_content_nnp_most:    
    if i[0] in ['2','3','4','드','햇','킴', '주']:
        w_content_nnp_most.remove(i)
        
w_content_nnp_dict = {}

# 딕셔너리 생성   
# wordcloud 만들기 위해서 딕셔너리 생성
for i in w_content_nnp_most:
    w_content_nnp_dict[i[0]] = i[1]
    
print(w_content_nnp_most)
```

    [('샤넬', 221), ('로즈', 94), ('레더', 83), ('구찌', 79), ('더블유', 77), ('브릴리언트', 63), ('프리미에르', 62), ('파리', 55), ('화이트', 53), ('하디드', 52), ('한국', 45), ('블랙', 45), ('티파니', 45), ('제니', 43), ('뉴진스', 43), ('차은우', 42), ('뉴욕', 42), ('서울', 42), ('힐', 42), ('백', 40), ('런웨이', 40), ('캐시미어', 40), ('리앙', 40), ('라운드', 33), ('아이코닉', 33), ('핑크', 32), ('벨라', 32), ('미국', 31), ('생로랑', 31), ('링', 31), ('사라', 29), ('맥', 28), ('베르사체', 28), ('페라가모', 27), ('아디다스', 27), ('루이', 27), ('펌프스', 25), ('프라다', 25), ('몽드', 25), ('주얼리', 24), ('알렉산더', 24), ('발렌티노', 24), ('보스턴', 24), ('다이애나', 24), ('비버', 23), ('오닉스', 23), ('웨스턴', 23), ('크러쉬', 22)]



```python
# elle 본문에서 고유 명사 빈도수

# 본문에서 추출한 키워드가 들어갈 리스트 
elle_content_nnp = []

for content in elle_content_list:
    for token in kiwi.tokenize(content):
        # 키워드 추출 
        if tuple(token)[1] in CHK_POS_NNP:
            elle_content_nnp.append(tuple(token)[0])
            
# 키워드 개수 세기
from collections import Counter
elle_content_nnp_counter = Counter(elle_content_nnp)
# 상위 키워드 50개 추출 
elle_content_nnp_most = elle_content_nnp_counter.most_common(50)

# 불용어 처리
for i in elle_content_nnp_most:    
    if i[0] in ['유튜브', '2','3','4','5','드','햇','킴']:
        elle_content_nnp_most.remove(i)

elle_content_nnp_dict = {}

# 딕셔너리 생성  
# wordclooud 이용하기 위해 딕셔너리 생성 
for i in elle_content_nnp_most:
    elle_content_nnp_dict[i[0]] = i[1]
        
print(elle_content_nnp_most)
```

    [('유튜브', 145), ('김나영', 127), ('스트라이프', 72), ('블랙', 70), ('레더', 62), ('삭스', 42), ('시크', 41), ('제니', 38), ('캡처', 38), ('강민경', 38), ('샤넬', 36), ('비비드', 34), ('캡쳐', 34), ('러버', 33), ('비니', 31), ('벨라', 31), ('하디드', 31), ('레트로', 31), ('김세정', 31), ('엘르', 30), ('런웨이', 29), ('기은세', 29), ('파리', 29), ('화이트', 28), ('크리스탈', 27), ('프라다', 26), ('베이식', 26), ('서울', 26), ('스니커즈', 25), ('나이키', 25), ('안소희', 25), ('핑크', 25), ('리파', 24), ('김고은', 23), ('예리', 23), ('두아', 22), ('브라운', 22), ('윤승아', 21), ('그레이', 21), ('인스타그램', 21), ('아이린', 20), ('백', 20), ('조이', 19), ('블레이저', 18), ('원피스', 18)]



```python
# 잡지 3사 본문에서 고유 명사 빈도수

# 잡지 3사 고유 명사 키워드 합치기 
sum_content_nnp = vogue_content_nnp + w_content_nnp + elle_content_nnp

sum_content_nnp_dict = {}

# 키워드 개수 세기
from collections import Counter
sum_content_nnp_counter = Counter(sum_content_nnp)
# 상위 키워드 50개 추출 
sum_content_nnp_most = sum_content_nnp_counter.most_common(50)


# 불용어 처리
for i in sum_content_nnp_most:    
    if i[0] in ['2','3','4','드','햇','킴', '주']:
        sum_content_nnp_most.remove(i)

# 딕셔너리 생성        
for i in sum_content_nnp_most:
    sum_content_nnp_dict[i[0]] = i[1]



print('3사 잡지사 본문 고유 명사: \n',sum_content_nnp_most)
print()
print('3사 잡지사 본문 고유 명사: \n',sum_content_nnp_dict)
```

    3사 잡지사 본문 고유 명사: 
     [('샤넬', 338), ('레더', 214), ('구찌', 173), ('블랙', 166), ('유튜브', 152), ('런웨이', 142), ('김나영', 139), ('하디드', 135), ('프라다', 120), ('뉴욕', 110), ('파리', 107), ('벨라', 105), ('스트라이프', 105), ('로즈', 99), ('시크', 98), ('제니', 94), ('백', 93), ('비버', 92), ('화이트', 92), ('에밀리', 88), ('미국', 86), ('서울', 82), ('블레이저', 82), ('헤일리', 80), ('비니', 77), ('더블유', 77), ('핑크', 75), ('힐', 75), ('한국', 74), ('인스타그램', 74), ('프리미에르', 73), ('테일러', 68), ('아디다스', 68), ('브릴리언트', 63), ('롱', 61), ('햇', 60), ('레트로', 60), ('캐시미어', 60), ('아이코닉', 59), ('비비드', 59), ('리파', 58), ('나이키', 57), ('러버', 57), ('두아', 56), ('티파니', 56)]
    
    3사 잡지사 본문 고유 명사: 
     {'샤넬': 338, '레더': 214, '구찌': 173, '블랙': 166, '유튜브': 152, '런웨이': 142, '김나영': 139, '하디드': 135, '프라다': 120, '뉴욕': 110, '파리': 107, '벨라': 105, '스트라이프': 105, '로즈': 99, '시크': 98, '제니': 94, '백': 93, '비버': 92, '화이트': 92, '에밀리': 88, '미국': 86, '서울': 82, '블레이저': 82, '헤일리': 80, '비니': 77, '더블유': 77, '핑크': 75, '힐': 75, '한국': 74, '인스타그램': 74, '프리미에르': 73, '테일러': 68, '아디다스': 68, '브릴리언트': 63, '롱': 61, '햇': 60, '레트로': 60, '캐시미어': 60, '아이코닉': 59, '비비드': 59, '리파': 58, '나이키': 57, '러버': 57, '두아': 56, '티파니': 56}


# JSON 파일 저장

## DataFrame --> json

앞에서 url에서 긁어온 제목과 본문을 데이터 프레임으로 만들고 이를 json 파일로 저장


```python
# 데이터 프레임 만들기

import pandas as pd 

# 각각의 딕셔너리 생성 
vogue_fashion = {}
w_fashion = {}
elle_fashion = {}

vogue_fashion['title'] = vogue_title_list
vogue_fashion['content'] = vogue_content_list

w_fashion['title'] = w_title_list
w_fashion['content'] = w_content_list

elle_fashion['title'] = elle_title_list
elle_fashion['content'] = elle_content_list

# 딕셔너리를 DataFrame으로 생성
vogue_df = pd.DataFrame(vogue_fashion)
w_df = pd.DataFrame(w_fashion)
elle_df = pd.DataFrame(elle_fashion)
```


```python
# vogue 데이터 프레임으로 저장
# orient='table'로 데이터프레임 구조 유지
vogue_df.to_json("vogue_df.json", orient = 'table')
```


```python
# w 데이터 프레임으로 저장 
# orient='table'로 데이터프레임 구조 유지
w_df.to_json("w_df.json", orient = 'table')
```


```python
# elle 데이터 프레임으로 저장 
# orient='table'로 데이터프레임 구조 유지
elle_df.to_json("elle_df.json", orient = 'table')
```


```python
# 저장한 json 파일이 잘 저장되었는지 확인하기

vogue_data = pd.read_json('vogue_df.json', orient='table')
w_data = pd.read_json('w_df.json', orient='table')
elle_data = pd.read_json('elle_df.json', orient='table')

vogue_data.head()
w_data.head()
elle_data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>content</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>톱모델의 패션 꿀팁 대방출해드립니다 #유튜브 #썽희안차</td>
      <td>\n\n\n\n\n\n\n \n\n \n\n\n\n크게 보기\n\n이전\n다음\n\...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>겨울 필수템 비니, 아직도 쓰던 대로만 써?</td>
      <td>\n\n\n\n\n\n\n \n\n\n\n크게 보기\n\n이전\n다음\n\n\n\n...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>겨울 ‘잇’템 #플러피햇 쇼핑리스트 8</td>
      <td>\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n크게 보기\n\n이전\n...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>연말 파티 룩으로도 제격! 과감할수록 쿨한 란제리 패션</td>
      <td>\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n크게 보기\n\n이전\n...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>한파 극복을 위한 ‘보온템’ #발라클라바 #쇼핑리스트</td>
      <td>\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n크게 보기\n\n이전\n...</td>
    </tr>
  </tbody>
</table>
</div>



## dict --> json

혹시 몰라서 딕셔너리를 json으로 바꿔도 보았다


```python
# vogue_json 파일 저장
with open("vogue_fashion.json", "w", encoding="UTF-8") as output_file: 
    json = ujson.dumps(vogue_fashion, ensure_ascii=False)
    print(json, file=output_file)
```


```python
# w_json 파일 저장
with open("w_fashion.json", "w", encoding="UTF-8") as output_file: 
    json = ujson.dumps(vogue_fashion, ensure_ascii=False)
    print(json, file=output_file)
```


```python
# elle_json 파일 저장
with open("elle_fashion.json", "w", encoding="UTF-8") as output_file: 
    json = ujson.dumps(vogue_fashion, ensure_ascii=False)
    print(json, file=output_file)
```

# 시각화

앞서 추출한 키워드들을 보기쉽게 matplotlib, wordcloud, squarfy로 시각화 해보겠다.

이 과정에서 중복되는 코드에 대한 주석은 여러번 나올 시 생략하겠다.

## matplotlib 

matplotlib은 엑셀의 그래프와 같이 여러 그래프 형태롤 표현할 수 있다.

### title and content NNP


```python
import matplotlib.pyplot as plt 
from matplotlib import rcParams
import seaborn as sns

# 한글 폰트와 사이즈 지정
# matplotlib 한글 깨짐 현상 발생해서
# 애플에서 제공하는 AppleGothic을 사용
rcParams['axes.titlepad'] = 20 
rcParams['font.family'] = 'AppleGothic'
rcParams['font.size'] = 18

# 색은 여러개 설정
COLORS = sns.color_palette("muted")

# vogue 본문 NNP 키워드를 시각화
plt.figure(figsize=(20, 10)) 
plt.title('VOGUE NNP', fontsize=32)
labels, counts = zip(*vogue_content_nnp_most)
plt.bar(labels, counts, color=COLORS)
# 글자를 세로로 보여줌 
plt.xticks(rotation=90)
plt.show()
```


    
![png](output_61_0.png)
    



```python
import matplotlib.pyplot as plt 
from matplotlib import rcParams
import seaborn as sns

rcParams['axes.titlepad'] = 20 
rcParams['font.family'] = 'AppleGothic'
rcParams['font.size'] = 18

COLORS = sns.color_palette("muted")

# W 본문 NNP 키워드 시각화
plt.figure(figsize=(20, 10)) 
plt.title('W NNP', fontsize=32)
labels, counts = zip(*w_content_nnp_most)
plt.bar(labels, counts, color=COLORS)
# 글자 세로로 보기
plt.xticks(rotation=90)
plt.show()
```


    
![png](output_62_0.png)
    



```python
import matplotlib.pyplot as plt 
from matplotlib import rcParams
import seaborn as sns

rcParams['axes.titlepad'] = 20 
rcParams['font.family'] = 'AppleGothic'
rcParams['font.size'] = 18

COLORS = sns.color_palette("muted")

# ELLE 본문 NNP 키워드 시각화
plt.figure(figsize=(20, 10)) 
plt.title('ELLE NNP', fontsize=32)
labels, counts = zip(*elle_content_nnp_most)
plt.bar(labels, counts, color=COLORS)
# 글자 세로로 보기
plt.xticks(rotation=90)
plt.show()
```


    
![png](output_63_0.png)
    



```python
# 잡지 3사 본문 고유 명사(NNP)

import matplotlib.pyplot as plt 
from matplotlib import rcParams
import seaborn as sns

rcParams['axes.titlepad'] = 20 
rcParams['font.family'] = 'AppleGothic'
rcParams['font.size'] = 18

# 컬러 여래개 
COLORS = sns.color_palette("muted")

# 잡지 3사 본문 NNP 키워드 시각화
plt.figure(figsize=(20, 10)) 
plt.title('잡지 3사 NNP', fontsize=32)
labels, counts = zip(*sum_content_nnp_most)
plt.bar(labels, counts, color=COLORS)
# 글자 세로로 보기
plt.xticks(rotation=90)
plt.show()
```


    
![png](output_64_0.png)
    



```python
# 잡지 3사 본문 고유 명사(NNP)

import matplotlib.pyplot as plt 
from matplotlib import rcParams
import seaborn as sns

rcParams['axes.titlepad'] = 20 
rcParams['font.family'] = 'AppleGothic'
rcParams['font.size'] = 18

# 컬러 여래개 
COLORS = sns.color_palette("muted")

# 잡지 3사 제목 NNP 키워드 시각화
plt.figure(figsize=(20, 10)) 
plt.title('잡지 3사 제목 NNP', fontsize=32)
labels, counts = zip(*sum_title_nnp_most)
plt.bar(labels, counts, color=COLORS)
# 글자 세로로 보기
plt.xticks(rotation=90)
plt.show()
```


    
![png](output_65_0.png)
    


### title and content NNG


```python
# VOGUE 잡지 본문 고유명사(NNG)

import matplotlib.pyplot as plt 
from matplotlib import rcParams
import seaborn as sns

rcParams['axes.titlepad'] = 20 
rcParams['font.family'] = 'AppleGothic'
rcParams['font.size'] = 18

COLORS = sns.color_palette("muted")

# vogue 본문 nng 키워드 시각화
plt.figure(figsize=(20, 10)) 
plt.title('VOGUE NNG', fontsize=32)
labels, counts = zip(*vogue_content_nng_most)
plt.bar(labels, counts, color=COLORS)
# x축 글자 세로로 보기
plt.xticks(rotation=90)
plt.show()
```


    
![png](output_67_0.png)
    



```python
# W 잡지 본문 고유명사(NNG)

import matplotlib.pyplot as plt 
from matplotlib import rcParams
import seaborn as sns

rcParams['axes.titlepad'] = 20 
rcParams['font.family'] = 'AppleGothic'
rcParams['font.size'] = 18

COLORS = sns.color_palette("muted")

# W 잡지 본문 NNG 키워드 시각화
plt.figure(figsize=(20, 10)) 
plt.title('W NNG', fontsize=32)
labels, counts = zip(*w_content_nng_most)
plt.bar(labels, counts, color=COLORS)
plt.xticks(rotation=90)
plt.show()
```


    
![png](output_68_0.png)
    



```python
# ELLE 잡지 본문 고유명사(NNG)

import matplotlib.pyplot as plt 
from matplotlib import rcParams
import seaborn as sns

rcParams['axes.titlepad'] = 20 
rcParams['font.family'] = 'AppleGothic'
rcParams['font.size'] = 18

COLORS = sns.color_palette("muted")

# ELLE 본문 NNG 키워드 시각화
plt.figure(figsize=(20, 10)) 
plt.title('ELLE NNG', fontsize=32)
labels, counts = zip(*elle_content_nng_most)
plt.bar(labels, counts, color=COLORS)
plt.xticks(rotation=90)
plt.show()
```


    
![png](output_69_0.png)
    



```python
# 잡지 3사 본문 고유명사(NNG)

import matplotlib.pyplot as plt 
from matplotlib import rcParams
import seaborn as sns

rcParams['axes.titlepad'] = 20 
rcParams['font.family'] = 'AppleGothic'
rcParams['font.size'] = 18

COLORS = sns.color_palette("muted")

# 잡지 3사 NNG 키워드 시각화
plt.figure(figsize=(20, 10)) 
plt.title('잡지 3사 NNG', fontsize=32)
labels, counts = zip(*sum_content_nng_most)
plt.bar(labels, counts, color=COLORS)
plt.xticks(rotation=90)
plt.show()
```


    
![png](output_70_0.png)
    



```python
# 잡지 3사 제목 일반 명사(NNG)

import matplotlib.pyplot as plt 
from matplotlib import rcParams
import seaborn as sns

rcParams['axes.titlepad'] = 20 
rcParams['font.family'] = 'AppleGothic'
rcParams['font.size'] = 18

COLORS = sns.color_palette("muted")

# 잡지 3사 제목 NNP 키워드 시각화
plt.figure(figsize=(20, 10)) 
plt.title('잡지 3사 제목 NNG', fontsize=32)
labels, counts = zip(*sum_title_nng_most)
plt.bar(labels, counts, color=COLORS)
plt.xticks(rotation=90)
plt.show()
```


    
![png](output_71_0.png)
    


## Wordcloud

### content NNP


```python
# VOGUE 잡지 본문 고유명사(NNP)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# 최대로 들어갈 단어 개수
N_WORDS = 50

# wordcloud 또한 한글 깨짐 이슈로 AppleGothic 이용
plt.rc('font', family='AppleGothic') 

# 배경화며 흰색, 폰트 가져오는 경로 등 설정 
wc = WordCloud(background_color="white", font_path = '/Library/Fonts/AppleGothic.ttf',
               max_words = N_WORDS, collocations=False, width=1920, height=1080)


# vogue 본문 nnp 시각화
wc.generate_from_frequencies(vogue_content_nnp_dict)
plt.title("VOGUE NNP", fontsize=18)
plt.imshow(wc, interpolation= 'bilinear')
plt.axis('off')
plt.gcf().set_size_inches(10, 8)
plt.show()
```


    
![png](output_74_0.png)
    



```python
# W 잡지 본문 고유명사(NNP)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

N_WORDS = 50

plt.rc('font', family='AppleGothic') 

wc = WordCloud(background_color="white", font_path = '/Library/Fonts/AppleGothic.ttf',
               max_words = N_WORDS, collocations=False, width=1920, height=1080)


# W 잡지 본문 NNP 시각화
wc.generate_from_frequencies(w_content_nnp_dict)
plt.title("W NNP", fontsize=18)
plt.imshow(wc, interpolation= 'bilinear')
plt.axis('off')
plt.gcf().set_size_inches(10, 8)
plt.show()
```


    
![png](output_75_0.png)
    



```python
# ELLE 잡지 본문 고유명사(NNP)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

N_WORDS = 50

plt.rc('font', family='AppleGothic') 

wc = WordCloud(background_color="white", font_path = '/Library/Fonts/AppleGothic.ttf',
               max_words = N_WORDS, collocations=False, width=1920, height=1080)


# ELLE 잡지 본문 NNP 시각화
wc.generate_from_frequencies(elle_content_nnp_dict)
plt.title("ELLE NNP", fontsize=18)
plt.imshow(wc, interpolation= 'bilinear')
plt.axis('off')
plt.gcf().set_size_inches(10, 8)
plt.show()
```


    
![png](output_76_0.png)
    



```python
# 잡지 3사 본문 고유명사(NNP)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

N_WORDS = 50

plt.rc('font', family='AppleGothic') 

wc = WordCloud(background_color="white", font_path = '/Library/Fonts/AppleGothic.ttf',
               max_words = N_WORDS, collocations=False, width=1920, height=1080)


# 잡지 3사 본문 NNP 시각화
wc.generate_from_frequencies(content_nnp_dict)
plt.title("잡지 3사 NNP", fontsize=18)
plt.imshow(wc, interpolation= 'bilinear')
plt.axis('off')
plt.gcf().set_size_inches(10, 8)
plt.show()
```


    
![png](output_77_0.png)
    



```python
# 잡지 3사 제목 고유명사(NNP)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# 제목 키워드 들어갈 개수 30개로 설정 
N_WORDS = 30

plt.rc('font', family='AppleGothic') 

wc = WordCloud(background_color="white", font_path = '/Library/Fonts/AppleGothic.ttf',
               max_words = N_WORDS, collocations=False, width=1920, height=1080)


# 잡지 3사 본문 NNP 시각화
wc.generate_from_frequencies(sum_title_nnp_dict)
plt.title("잡지 3사 제목 NNP", fontsize=18)
plt.imshow(wc, interpolation= 'bilinear')
plt.axis('off')
plt.gcf().set_size_inches(10, 8)
plt.show()
```


    
![png](output_78_0.png)
    


### content NNG


```python
# VOGUE 잡지 본문 일반 명사(NNG)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

N_WORDS = 50

plt.rc('font', family='AppleGothic') 

wc = WordCloud(background_color="white", font_path = '/Library/Fonts/AppleGothic.ttf',
               max_words = N_WORDS, collocations=False, width=1920, height=1080)


wc.generate_from_frequencies(vogue_content_nng_dict)
plt.title("VOGUE NNG", fontsize=18)
plt.imshow(wc, interpolation= 'bilinear')
plt.axis('off')
plt.gcf().set_size_inches(10, 8)
plt.show()
```


    
![png](output_80_0.png)
    



```python
# W 잡지 본문 일반 명사(NNG)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

N_WORDS = 50

plt.rc('font', family='AppleGothic') 

wc = WordCloud(background_color="white", font_path = '/Library/Fonts/AppleGothic.ttf',
               max_words = N_WORDS, collocations=False, width=1920, height=1080)


wc.generate_from_frequencies(w_content_nng_dict)
plt.title("W NNG", fontsize=18)
plt.imshow(wc, interpolation= 'bilinear')
plt.axis('off')
plt.gcf().set_size_inches(10, 8)
plt.show()
```


    
![png](output_81_0.png)
    



```python
# W 잡지 본문 일반 명사(NNG)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

N_WORDS = 50

plt.rc('font', family='AppleGothic') 

wc = WordCloud(background_color="white", font_path = '/Library/Fonts/AppleGothic.ttf',
               max_words = N_WORDS, collocations=False, width=1920, height=1080)


wc.generate_from_frequencies(elle_content_nng_dict)
plt.title("ELLE NNG", fontsize=18)
plt.imshow(wc, interpolation= 'bilinear')
plt.axis('off')
plt.gcf().set_size_inches(10, 8)
plt.show()
```


    
![png](output_82_0.png)
    



```python
# 잡지 3사 본문 일반 명사(NNG)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

N_WORDS = 50

plt.rc('font', family='AppleGothic') 

wc = WordCloud(background_color="white", font_path = '/Library/Fonts/AppleGothic.ttf',
               max_words = N_WORDS, collocations=False, width=1920, height=1080)


wc.generate_from_frequencies(sum_content_nng_dict)
plt.title("잡지 3사 NNG", fontsize=18)
plt.imshow(wc, interpolation= 'bilinear')
plt.axis('off')
plt.gcf().set_size_inches(10, 8)
plt.show()
```


    
![png](output_83_0.png)
    



```python
# 잡지 3사 제목 일반 명사(NNG)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

N_WORDS = 50

plt.rc('font', family='AppleGothic') 

wc = WordCloud(background_color="white", font_path = '/Library/Fonts/AppleGothic.ttf',
               max_words = N_WORDS, collocations=False, width=1920, height=1080)

# 잡지 3사 제목 NNG 시각화
wc.generate_from_frequencies(sum_title_nng_dict)
plt.title("잡지 3사 제목 NNG", fontsize=18)
plt.imshow(wc, interpolation= 'bilinear')
plt.axis('off')
plt.gcf().set_size_inches(10, 8)
plt.show()
```


    
![png](output_84_0.png)
    


## squarify

### content NNP


```python
# VOGUE 잡지 본문 고유명사

import sys
import squarify
import matplotlib.pyplot as plt
import seaborn as sns

N_WORDS = 50

# 컬러는 파스텔 색깔들로 사용
COLORS = sns.color_palette("pastel")
plt.rc("font", family='AppleGothic')

# vogue 본문 NNP 키워드 시각화
word_count_pairs = vogue_content_nnp_most
words, counts = zip(*vogue_content_nnp_most)

# 키워드 개수 많은 순으로 정렬
squarify.plot(sizes=counts, label=words, color=COLORS, alpha=0.9, text_kwargs={'fontsize': 15})
plt.axis("off")
plt.title('VOGUE NNP', fontsize=18)
plt.gcf().set_size_inches(16, 10)
plt.show()
```


    
![png](output_87_0.png)
    



```python
# W 잡지 고유명사

import sys
import squarify
import matplotlib.pyplot as plt
import seaborn as sns

N_WORDS = 50

COLORS = sns.color_palette("pastel")
plt.rc("font", family='AppleGothic')


word_count_pairs = w_content_nnp_most
words, counts = zip(*w_content_nnp_most)

squarify.plot(sizes=counts, label=words, color=COLORS, alpha=0.9, text_kwargs={'fontsize': 15})
plt.axis("off")
plt.title('W NNP', fontsize=18)
plt.gcf().set_size_inches(16, 10)
plt.show()
```


    
![png](output_88_0.png)
    



```python
# ELLE 잡지 본문 고유 명사

import sys
import squarify
import matplotlib.pyplot as plt
import seaborn as sns

N_WORDS = 50

COLORS = sns.color_palette("pastel")
plt.rc("font", family='AppleGothic')

# ELLE 잡지 본문 NNP 시각화
word_count_pairs = elle_content_nnp_most
words, counts = zip(*elle_content_nnp_most)

squarify.plot(sizes=counts, label=words, color=COLORS, alpha=0.9, text_kwargs={'fontsize': 15})
plt.axis("off")
plt.title('ELLE NNP', fontsize=18)
plt.gcf().set_size_inches(16, 10)
plt.show()
```


    
![png](output_89_0.png)
    



```python
# 잡지 3사 본문 고유 명사

import sys
import squarify
import matplotlib.pyplot as plt
import seaborn as sns

N_WORDS = 50

COLORS = sns.color_palette("pastel")
plt.rc("font", family='AppleGothic')

# 잡지 3사 본문 NNP 시각화
word_count_pairs = sum_content_nnp_most
words, counts = zip(*sum_content_nnp_most)

squarify.plot(sizes=counts, label=words, color=COLORS, alpha=0.9, text_kwargs={'fontsize': 15})
plt.axis("off")
plt.title('잡지 3사 NNP', fontsize=18)
plt.gcf().set_size_inches(16, 10)
plt.show()
```


    
![png](output_90_0.png)
    



```python
# 잡지 3사 제목 고유 명사

import sys
import squarify
import matplotlib.pyplot as plt
import seaborn as sns

N_WORDS = 50

COLORS = sns.color_palette("pastel")
plt.rc("font", family='AppleGothic')

# 잡지 3사 제목 NNP 시각화
word_count_pairs = sum_title_nnp_most
words, counts = zip(*sum_title_nnp_most)

squarify.plot(sizes=counts, label=words, color=COLORS, alpha=0.9, text_kwargs={'fontsize': 15})
plt.axis("off")
plt.title('잡지 3사 제목 NNP', fontsize=18)
plt.gcf().set_size_inches(16, 10)
plt.show()
```


    
![png](output_91_0.png)
    


### content NNG


```python
# VOGUE 잡지 본문 NNG

import sys
import squarify
import matplotlib.pyplot as plt
import seaborn as sns

N_WORDS = 50

COLORS = sns.color_palette("pastel")
plt.rc("font", family='AppleGothic')

# VOGUE 잡지 본문 NNG 시각화
word_count_pairs = vogue_content_nng_most
words, counts = zip(*vogue_content_nng_most)

squarify.plot(sizes=counts, label=words, color=COLORS, alpha=0.9, text_kwargs={'fontsize': 15})
plt.axis("off")
plt.title('VOGUE NNG', fontsize=18)
plt.gcf().set_size_inches(16, 10)
plt.show()
```


    
![png](output_93_0.png)
    



```python
# W 잡지 본문 NNG

import sys
import squarify
import matplotlib.pyplot as plt
import seaborn as sns

N_WORDS = 50

COLORS = sns.color_palette("pastel")
plt.rc("font", family='AppleGothic')

# W 잡지 본문 NNG 시각화
word_count_pairs = w_content_nng_most
words, counts = zip(*w_content_nng_most)

squarify.plot(sizes=counts, label=words, color=COLORS, alpha=0.9, text_kwargs={'fontsize': 15})
plt.axis("off")
plt.title('W NNG', fontsize=18)
plt.gcf().set_size_inches(16, 10)
plt.show()
```


    
![png](output_94_0.png)
    



```python
# ELLE 잡지 본문 일반 명사(NNG)

import sys
import squarify
import matplotlib.pyplot as plt
import seaborn as sns

N_WORDS = 50

COLORS = sns.color_palette("pastel")
plt.rc("font", family='AppleGothic')

# ELLE 잡지 본문 일반 명사(NNG) 시각화
word_count_pairs = elle_content_nng_most
words, counts = zip(*elle_content_nng_most)

squarify.plot(sizes=counts, label=words, color=COLORS, alpha=0.9, text_kwargs={'fontsize': 15})
plt.axis("off")
plt.title('ELLE NNG', fontsize=18)
plt.gcf().set_size_inches(16, 10)
plt.show()
```


    
![png](output_95_0.png)
    



```python
# 잡지 3사 본문 일반 명사(NNG)

import sys
import squarify
import matplotlib.pyplot as plt
import seaborn as sns

N_WORDS = 50

COLORS = sns.color_palette("pastel")
plt.rc("font", family='AppleGothic')

# 잡지 3사 본문 일반 명사(NNG) 시각화
word_count_pairs = sum_content_nng_most
words, counts = zip(*sum_content_nng_most)

squarify.plot(sizes=counts, label=words, color=COLORS, alpha=0.9, text_kwargs={'fontsize': 15})
plt.axis("off")
plt.title('잡지 3사 NNG', fontsize=18)
plt.gcf().set_size_inches(16, 10)
plt.show()
```


    
![png](output_96_0.png)
    



```python
# 잡지 3사 제목 일반 명사(NNG)

import sys
import squarify
import matplotlib.pyplot as plt
import seaborn as sns

N_WORDS = 50

COLORS = sns.color_palette("pastel")
plt.rc("font", family='AppleGothic')

# 잡지 3사 제목 일반 명사(NNG) 시각화
word_count_pairs = sum_title_nng_most
words, counts = zip(*sum_title_nng_most)

squarify.plot(sizes=counts, label=words, color=COLORS, alpha=0.9, text_kwargs={'fontsize': 15})
plt.axis("off")
plt.title('잡지 3사 제목 NNG', fontsize=18)
plt.gcf().set_size_inches(16, 10)
plt.show()
```


    
![png](output_97_0.png)
    


# 결론 

## 결과 해석

이렇게 VOGUE, W, ELLE 세 잡지사의 1~2개월 가량의 게시물을 크롤링해 일반 명사와 고유 명사의 키워드들을 추출해 빈도수를 matplotlib, wordcloud, squarify로 시각화해서 살펴보았다. 잡지 3사의 키워드를 합쳐 살펴본 그래프들을 보면 일반 명사, 고유 명사에서 보여주는 단어들은 각각 다르다. 일반 명사에서는 보통 패션 아이템, 계절 같은 단어들이 도출되고, 고유 명사에는 셀럽, 브랜드, 의류 종류 같은 단어들이 각각 나왔다. 

고유 명사 키워드에서 요즘 유행하는 브랜드인 아디다스, 샤넬과 인기있는 셀럽인 뉴진스, 제니 등이 나왔다. 레트로 같은 단어와 발라클라바라는 패션용품도 많이 언급되었다. 그래서 고유 명사 키워드는 요즘 유행하는 브랜드가 무엇인지, 인기있는 셀럽의 코디를 참고하거나, 요즘의 트렌드, 요즘 많이 사용하는 패션 용품 등을 알아볼 때 사용하는 것이 유용해보인다.

일반 명사 키워드에서는 코트, 패딩과 같이 겨울과 관련된 패션 용품과 요즘 시기에 어울리는 컬러 키워드가 많이 담겨있다.일반 명사 키워드를 참조 할 때에는 요즘 시기에 필요할 패션 용품을 찾아보거나 요즘 시기에 어울리는 컬라가 뭔지 알고 싶을 때 사용하는 것이 유용보인다.  

결과적으로 일반 명사, 고유 명사 키워드들은 각각 다른 결을 가지고 있어, 이를 참조할 때에는 하나의 편중해서 보기 보다는 같이 보면서 사용하는 것이 트렌드를 살펴보는데 더 유용할 것이다.

## 한계점 및 느낀점

일반 명사, 고유 명사 키워드를 살펴보면 일반 명사 키워드에서 그렇게 좋은 키워드를 뽑아낼 수가 없었다. 단순한 명사만 추출되었다. 트렌드를 간접적으로 느낄 수 있는 명사는 모두 고유 명사에 들어가있었다. 고유 명사 키워드에서 셀럽 뉴진스, 제니나 요즘 다시 유행하는 아디다스, 꺾이지 않는 명품 샤넬, 이번 겨울 철 유행하는 패션 용품 발라클라바 등 이런 중요한 키워드는 모두 고유 명사에 들어가 있었다. 프로젝트 진행 시 일반 명사를 아예 배제하고 진행하는 것은 많은 데이터를 잃을 것이라 판단하여 고유 명사와 일반 명사에서 키워드를 추출했지만 , 일반 명사에서 좋은 키워드들을 뽑아내지 못해 아쉽다.

또한 잡지 사 특성상 여성이 주고객이라, 대표적인 VOGUE, W, ELLE에서도 남성 관련 게시글이 많이 없어 남성 패션 트렌드는 맞지 않을 가능성이 있다. 그러나 여성 잡지와 남성 잡지의 키워드를 합치게 되면 키워드들이 지닌 의미가 희석된다고 생각해서 여성 잡지만 사용하였다. 그래서 이번 프로젝트에서 패션 트렌드는 주로 여성의 패션과 관련 되어있을 것이다. 다음 비슷한 프로젝트에서는 남성 잡지와 여성 잡지 각각에서 크롤링을 해서 성별 별 패션 트렌드를 예측할 수 있도록 할 것이다. 
